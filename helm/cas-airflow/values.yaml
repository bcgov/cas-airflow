route:
  clusterDomain: apps.silver.devops.gov.bc.ca

dynamicDagStorage:
  pvcName: cas-airflow-dynamic-dags-pvc
  accessMode: ReadWriteMany
  size: 1Gi
  storageClassName: netapp-file-standard

# The name of the ArtifactoryServiceAccount object created in the cas-provision chart
artifactoryServiceAccount: cas-artifact-download

# The values we need to override from the airflow defaults
airflow:
  uid: 1002490000
  gid: 1002490000
  executor: KubernetesExecutor

  # This should match the mounting point of the cas-airflow-dynamic-dags-pvc PVC
  dynamicDagsPath: /opt/airflow/dags/dynamic

  defaultAirflowRepository: artifacts.developer.gov.bc.ca/google-docker-remote/ggl-cas-storage/cas-airflow
  # Will be overridden by the git commit sha
  defaultAirflowTag: "to_override"

  airflowVersion: "2.9.3"

  # Enable RBAC (default on most clusters these days)
  rbac:
    create: true
    createSCCRoleBinding: false

  # Environment variables for all airflow containers
  env:
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: /gcs/credentials.json

  #
  extraEnv: |
    - name: DYNAMIC_DAGS_PATH
      value: {{ .Values.dynamicDagsPath }}

  # Images
  images:
    airflow:
      pullPolicy: Always
    pod_template:
      pullPolicy: Always

  # Secrets for all airflow containers
  secret:
    - envName: DEFAULT_USER_PASS
      secretName: airflow-default-user-password
      secretKey: default-user-pass
    - envName: PGPASS
      secretName: cas-airflow-patroni
      secretKey: password-superuser
    - envName: AIRFLOW_NAMESPACE
      secretName: cas-namespaces
      secretKey: airflow-namespace
    - envName: GGIRCS_NAMESPACE
      secretName: cas-namespaces
      secretKey: ggircs-namespace
    - envName: CIIP_NAMESPACE
      secretName: cas-namespaces
      secretKey: ciip-namespace
    - envName: CIF_NAMESPACE
      secretName: cas-namespaces
      secretKey: cif-namespace
    - envName: GH_CLIENT_ID
      secretName: airflow-oauth
      secretKey: clientId
    - envName: GH_CLIENT_SECRET
      secretName: airflow-oauth
      secretKey: clientSecret

  # Airflow database config
  data:
    metadataSecretName: cas-airflow-metadata
    resultBackendSecretName: cas-airflow-metadata

    metadataConnection:
      user: postgres
      pass: $PGPASS
      host: ~
      port: 5432
      db: postgres
      sslmode: disable
    resultBackendConnection:
      user: postgres
      pass: $PGPASS
      host: cas-airflow-patroni
      port: 5432
      db: postgres
      sslmode: disable
      protocol: postgresql

  # Airflow webserver settings
  webserverSecretKeySecretName: cas-airflow-webserver-secret

  webserver:
    replicas: 2
    resources:
      requests:
        cpu: 50m
        memory: 700Mi
      limits:
        cpu: 500m
        memory: 2Gi
    defaultUser:
      enabled: true
      role: Admin
      username: cas-airflow-admin
      email: admin@example.com
      firstName: admin
      lastName: user
      password: "$(DEFAULT_USER_PASS)"
    startupProbe:
      timeoutSeconds: 120
      periodSeconds: 60

  # Airflow scheduler settings
  scheduler:
    resources:
      limits:
        cpu: 1000m
    extraVolumes:
      - name: cas-airflow-dynamic-dags
        persistentVolumeClaim:
          claimName: cas-airflow-dynamic-dags-pvc
    extraVolumeMounts:
      - name: cas-airflow-dynamic-dags
        mountPath: /opt/airflow/dags/dynamic

  postgresql:
    enabled: false

  triggerer:
    enabled: false

  config:
    core:
      max_active_tasks_per_dag: 5
      max_active_runs_per_dag: 1
      min_serialized_dag_update_interval: 10
      min_serialized_dag_fetch_interval: 5
      dags_are_paused_at_creation: False
    logging:
      # The log level should not be decreased to INFO/DEBUG,
      # or only temporarily, as the airflow sheduler is very verbose,
      # putting a strain on the shared logging system
      # with WARN, the webserver currently prints a large json schema
      logging_level: ERROR
      remote_logging: "True"
      remote_base_log_folder: ~
      remote_log_conn_id: gcs_logs
    api:
      auth_backend: airflow.api.auth.backend.basic_auth
    webserver:
      web_server_worker_timeout: 300
      workers: 2
      # Prevent airflow to be rendered in an external web frame
      x_frame_enabled: "False"
      # Dev, Test and Prod are all external facing
      warn_deployment_exposure: "False"

    smtp:
      smtp_host: apps.smtp.gov.bc.ca
      smtp_starttls: false
      smtp_mail_from: cas-airflow@gov.bc.ca

    scheduler:
      min_file_process_interval: 30

    kubernetes_executor:
      run_as_user: "{{ .Values.uid }}"
      delete_worker_pods: "True"

  # Git sync
  dags:
    persistence:
      enabled: false
    gitSync:
      enabled: false

  statsd:
    enabled: false
# Configuration for cas-postgres subchart
cas-postgres:
  patroni:
    resources:
      limits:
        cpu: 500m
        memory: 2Gi
      requests:
        cpu: 50m
        memory: 600Mi
    persistentVolume:
      size: 10Gi
      storageClass: netapp-block-standard
    replicaCount: 3
    env:
      ALLOW_NOSSL: true
      USE_WALG_BACKUP: "true"
      USE_WALG_RESTORE: "true"
    walE:
      enable: true
      gcsBucket: ~
      # kubernetesSecret should be gcp-{{ namespace }}-{{ gcs.bucketSuffix }}-service-account-key
      kubernetesSecret: ~
    # options below are required if walE.enable is true
  namespace: ~
  gcs:
    bucketSuffix: ~

devops:
  image:
    repository: hashicorp/terraform
    pullPolicy: Always
    tag: "1.4.6"

  resources:
    limits:
      cpu: 1000m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 64Mi

terraform-bucket-provision:
  terraform:
    namespace_apps: '["airflow-backups", "airflow-logs"]'
