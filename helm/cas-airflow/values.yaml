route:
  clusterDomain: apps.silver.devops.gov.bc.ca

gcpLogsUpload:
  enable: true
  bucketSuffix: airflow-logs
  image:
    casShelf:
      repository: artifacts.developer.gov.bc.ca/google-docker-remote/ggl-cas-storage/cas-shelf-tfe-add-app
      pullPolicy: Always
      tag: "0.1"

dynamicDagStorage:
  pvcName: cas-airflow-dynamic-dags-pvc
  accessMode: ReadWriteMany
  size: 1Gi
  storageClassName: netapp-file-standard

# The name of the ArtifactoryServiceAccount object created in the cas-provision chart
artifactoryServiceAccount: cas-artifact-download

# The values we need to override from the airflow defaults
airflow:
  uid: 1002490000
  gid: 1002490000
  executor: KubernetesExecutor

  # This should match the mounting point of the cas-airflow-dynamic-dags-pvc PVC
  dynamicDagsPath: /opt/airflow/dags/dynamic

  defaultAirflowRepository: artifacts.developer.gov.bc.ca/google-docker-remote/ggl-cas-storage/cas-airflow
  # Will be overridden by the git commit sha
  defaultAirflowTag: "to_override"

  airflowVersion: "2.4.2"

  # Enable RBAC (default on most clusters these days)
  rbac:
    create: true
    creaateSCCRoleBinding: false

  # Environment variables for all airflow containers
  env:
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: /gcs/credentials.json

  #
  extraEnv: |
    - name: DYNAMIC_DAGS_PATH
      value: {{ .Values.dynamicDagsPath }}

  # Images
  images:
    airflow:
      pullPolicy: Always
    pod_template:
      pullPolicy: Always

  # Secrets for all airflow containers
  secret:
    - envName: PGPASS
      secretName: cas-airflow-patroni
      secretKey: password-superuser
    - envName: DEFAULT_USER_PASS
      secretName: airflow-default-user-password
      secretKey: default-user-pass
    - envName: AIRFLOW_NAMESPACE
      secretName: cas-namespaces
      secretKey: airflow-namespace
    - envName: GGIRCS_NAMESPACE
      secretName: cas-namespaces
      secretKey: ggircs-namespace
    - envName: CIIP_NAMESPACE
      secretName: cas-namespaces
      secretKey: ciip-namespace
    - envName: CIF_NAMESPACE
      secretName: cas-namespaces
      secretKey: cif-namespace

  # Airflow database config
  data:
    metadataSecretName: cas-airflow-metadata
    resultBackendSecretName: cas-airflow-metadata

    metadataConnection:
      user: postgres
      pass: $PGPASS
      host: ~
      port: 5432
      db: postgres
      sslmode: disable
    resultBackendConnection:
      user: postgres
      pass: $PGPASS
      host: cas-airflow-patroni
      port: 5432
      db: postgres
      sslmode: disable
      protocol: postgresql

  # Airflow webserver settings
  webserver:
    replicas: 2
    resources:
      requests:
        cpu: 50m
        memory: 700Mi
      limits:
        cpu: 500m
        memory: 2Gi
    # defaultUser:
    #   enabled: true
    #   role: Admin
    #   username: cas-airflow-admin
    #   email: admin@example.com
    #   firstName: admin
    #   lastName: user
    #   password: "$(DEFAULT_USER_PASS)"
    webserverConfig: |-
      from airflow.www.security import AirflowSecurityManager
      import logging
      from typing import Dict, Any, List, Union
      import os

      log = logging.getLogger(__name__)
      log.setLevel(os.getenv("AIRFLOW__LOGGING__FAB_LOGGING_LEVEL", "INFO"))

      FAB_ADMIN_ROLE = "Admin"
      FAB_VIEWER_ROLE = "Viewer"
      FAB_PUBLIC_ROLE = "Public"  # The "Public" role is given no permissions
      # This is the cas-developers team id
      TEAM_ID_A_FROM_GITHUB = 3204518


      def team_parser(team_payload: Dict[str, Any]) -> List[int]:
          # Parse the team payload from Github however you want here.
          printme = [team["name"] for team in team_payload]
          print(f"team ids: {printme}")
          return [team["id"] for team in team_payload]


      def map_roles(team_list: List[int]) -> List[str]:
          # Associate the team IDs with Roles here.
          # The expected output is a list of roles that FAB will use to Authorize the user.

          team_role_map = {
              TEAM_ID_A_FROM_GITHUB: FAB_ADMIN_ROLE,
          }
          printthistoo = list(set(team_role_map.get(team, FAB_PUBLIC_ROLE) for team in team_list))
          print(' '.join(printthistoo))
          return list(set(team_role_map.get(team, FAB_PUBLIC_ROLE) for team in team_list))


      class GithubTeamAuthorizer(AirflowSecurityManager):

          # In this example, the oauth provider == 'github'.
          # If you ever want to support other providers, see how it is done here:
          # https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/security/manager.py#L550
          def get_oauth_user_info(
              self, provider: str, resp: Any
          ) -> Dict[str, Union[str, List[str]]]:

              # Creates the user info payload from Github.
              # The user previously allowed your app to act on thier behalf,
              #   so now we can query the user and teams endpoints for their data.
              # Username and team membership are added to the payload and returned to FAB.

              remote_app = self.appbuilder.sm.oauth_remotes[provider]
              me = remote_app.get("user")
              user_data = me.json()
              team_data = remote_app.get("user/teams")
              teams = team_parser(team_data.json())
              roles = map_roles(teams)
              
              # MIKE
              print(f"User info from Github: {user_data}\n" f"Team info from Github: {teams}")
              print(f"Team data: {team_data}")
              print(f"Team data: {teams}")
              
              log.debug(
                  f"User info from Github: {user_data}\n" f"Team info from Github: {teams}"
              )
              x={"username": "github_" + user_data.get("login"), "role_keys": roles}
              print(f"auth class return: {x}")
              return {"username": "github_" + user_data.get("login"), "role_keys": roles}

      #######################################
      # Actual `webserver_config.py`
      #######################################
      from flask_appbuilder.security.manager import AUTH_OAUTH
      import os

      AUTH_TYPE = AUTH_OAUTH
      AUTH_ROLES_SYNC_AT_LOGIN = True  # Checks roles on every login
      AUTH_USER_REGISTRATION = (
          True  # allow users who are not already in the FAB DB to register
      )
      # Make sure to replace this with the path to your security manager class
      FAB_SECURITY_MANAGER_CLASS = "webserver_config.GithubTeamAuthorizer"

      # If you wish, you can add multiple OAuth providers.
      OAUTH_PROVIDERS = [
          {
              "name": "github",
              "icon": "fa-github",
              "token_key": "access_token",
              "remote_app": {
                  "client_id": "",
                  "client_secret": "",
                  "api_base_url": "https://api.github.com",
                  "client_kwargs": {"scope": "read:org, read:user"},
                  "access_token_url": "https://github.com/login/oauth/access_token",
                  "authorize_url": "https://github.com/login/oauth/authorize",
                  "request_token_url": None,
              },
          },
      ]

  # Airflow scheduler settings
  scheduler:
    resources:
      limits:
        cpu: 1000m
    extraVolumes:
      - name: cas-airflow-dynamic-dags
        persistentVolumeClaim:
          claimName: cas-airflow-dynamic-dags-pvc
    extraVolumeMounts:
      - name: cas-airflow-dynamic-dags
        mountPath: /opt/airflow/dags/dynamic

  postgresql:
    enabled: false

  triggerer:
    enabled: false

  config:
    core:
      dag_concurrency: 5
      max_active_runs_per_dag: 1
      min_serialized_dag_update_interval: 10
      min_serialized_dag_fetch_interval: 5
      dags_are_paused_at_creation: False
    logging:
      # The log level should not be decreased to INFO/DEBUG,
      # or only temporarily, as the airflow sheduler is very verbose,
      # putting a strain on the shared logging system
      # with WARN, the webserver currently prints a large json schema
      logging_level: ERROR
      remote_logging: "True"
      remote_base_log_folder: ~
      remote_log_conn_id: gcs_logs
    api:
      auth_backend: airflow.api.auth.backend.basic_auth
    webserver:
      username: "cas-airflow-admin"
      password: "$(DEFAULT_USER_PASS)"
      web_server_worker_timeout: 300
      workers: 2

    smtp:
      smtp_host: apps.smtp.gov.bc.ca
      smtp_starttls: false
      smtp_mail_from: cas-airflow@gov.bc.ca

    scheduler:
      min_file_process_interval: 30

    kubernetes:
      run_as_user: "{{ .Values.uid }}"
      delete_worker_pods: "True"

  # Git sync
  dags:
    persistence:
      enabled: false
    gitSync:
      enabled: false

  statsd:
    enabled: false
# Configuration for cas-postgres subchart
cas-postgres:
  patroni:
    resources:
      limits:
        cpu: 500m
        memory: 2Gi
      requests:
        cpu: 50m
        memory: 600Mi
    persistentVolume:
      size: 10Gi
      storageClass: netapp-block-standard
    replicaCount: 3
    env:
      ALLOW_NOSSL: true
      USE_WALG_BACKUP: "true"
      USE_WALG_RESTORE: "true"
    walE:
      enable: true
      gcsBucket: ~
      # kubernetesSecret should be gcp-{{ namespace }}-{{ gcs.bucketSuffix }}-service-account-key
      kubernetesSecret: ~
    # options below are required if walE.enable is true
  namespace: ~
  gcs:
    bucketSuffix: ~
