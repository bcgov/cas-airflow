route:
  clusterDomain: apps.silver.devops.gov.bc.ca

dynamicDagStorage:
  pvcName: cas-airflow-dynamic-dags-pvc
  accessMode: ReadWriteMany
  size: 1Gi
  storageClassName: netapp-file-standard

# The name of the ArtifactoryServiceAccount object created in the cas-provision chart
artifactoryServiceAccount: cas-artifact-download

database:
  clusterReleaseName: cas-airflow-db

# The values we need to override from the airflow defaults
airflow:
  uid: 1002490000
  gid: 1002490000
  executor: KubernetesExecutor

  # This should match the mounting point of the cas-airflow-dynamic-dags-pvc PVC
  dynamicDagsPath: /opt/airflow/dags/dynamic

  defaultAirflowRepository: ghcr.io/bcgov/cas-airflow
  # Will be overridden by the git commit sha
  defaultAirflowTag: "to_override"

  airflowVersion: "3.1.0"

  # Enable RBAC (default on most clusters these days)
  rbac:
    create: true
    createSCCRoleBinding: false

  # Environment variables for all airflow containers
  env:
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: /gcs/credentials.json

  #
  extraEnv: |
    - name: DYNAMIC_DAGS_PATH
      value: {{ .Values.dynamicDagsPath }}

  # Images
  images:
    airflow:
      pullPolicy: Always
    pod_template:
      pullPolicy: Always

  # Secrets for all airflow containers
  secret:
    - envName: DEFAULT_USER_PASS
      secretName: airflow-default-user-password
      secretKey: default-user-pass
    - envName: PGPASS
      secretName: cas-airflow-db-cas-postgres-cluster-pguser-airflow
      secretKey: password
    - envName: HOST
      secretName: cas-airflow-db-cas-postgres-cluster-pguser-airflow
      secretKey: pgbouncer-host
    - envName: PGBOUNCER_URI
      secretName: cas-airflow-db-cas-postgres-cluster-pguser-airflow
      secretKey: pgbouncer-uri
    - envName: AIRFLOW_NAMESPACE
      secretName: cas-namespaces
      secretKey: airflow-namespace
    - envName: GGIRCS_NAMESPACE
      secretName: cas-namespaces
      secretKey: ggircs-namespace
    - envName: CIIP_NAMESPACE
      secretName: cas-namespaces
      secretKey: ciip-namespace
    - envName: CIF_NAMESPACE
      secretName: cas-namespaces
      secretKey: cif-namespace
    - envName: GH_CLIENT_ID
      secretName: airflow-oauth
      secretKey: clientId
    - envName: GH_CLIENT_SECRET
      secretName: airflow-oauth
      secretKey: clientSecret
    - envName: BCIERS_NAMESPACE
      secretName: cas-namespaces
      secretKey: obps-namespace

  # Airflow database config
  data:
    metadataSecretName: cas-airflow-metadata
    resultBackendSecretName: cas-airflow-result-backend

    metadataConnection:
      user: airflow
      pass: $PGPASS
      host: $HOST
      port: 5432
      db: postgres
      sslmode: disable
    resultBackendConnection:
      user: airflow
      pass: $PGPASS
      host: $HOST
      port: 5432
      db: postgres
      sslmode: disable
      protocol: postgresql

  # Airflow api server settings
  apiSecretKeySecretName: cas-airflow-api-secret

  apiServer:
    replicas: 2
    resources:
      requests:
        cpu: 50m
        memory: 700Mi
      limits:
        cpu: 500m
        memory: 2Gi
    defaultUser:
      enabled: true
      role: Admin
      username: cas-airflow-admin
      email: admin@example.com
      firstName: admin
      lastName: user
      password: "$(DEFAULT_USER_PASS)"
    startupProbe:
      timeoutSeconds: 120
      periodSeconds: 60

  # createUserJob.args param uses the webserver.defaultUser values as of Airflow 3.1.0,
  # So we need to override them here instead of in apiServer.defaultUser
  webserver:
    defaultUser:
      enabled: true
      role: Admin
      username: cas-airflow-admin
      email: admin@example.com
      firstName: admin
      lastName: user
      password: "$(DEFAULT_USER_PASS)"

  # Airflow scheduler settings
  scheduler:
    resources:
      limits:
        cpu: 1000m
    extraVolumes:
      - name: cas-airflow-dynamic-dags
        persistentVolumeClaim:
          claimName: cas-airflow-dynamic-dags-pvc
    extraVolumeMounts:
      - name: cas-airflow-dynamic-dags
        mountPath: /opt/airflow/dags/dynamic
    env:
      - name: PYTHONPATH
        value: "/opt/airflow/dags"

  # Airflow dag processor settings
  dagProcessor:
    resources:
      limits:
        cpu: 1000m
    extraVolumes:
      - name: cas-airflow-dynamic-dags
        persistentVolumeClaim:
          claimName: cas-airflow-dynamic-dags-pvc
    extraVolumeMounts:
      - name: cas-airflow-dynamic-dags
        mountPath: /opt/airflow/dags/dynamic
    env:
      - name: PYTHONPATH
        value: "/opt/airflow/dags"

  postgresql:
    enabled: false

  triggerer:
    enabled: false

  config:
    core:
      max_active_tasks_per_dag: 5
      max_active_runs_per_dag: 1
      min_serialized_dag_update_interval: 10
      min_serialized_dag_fetch_interval: 5
      dags_are_paused_at_creation: False
      auth_manager: "airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager"
    logging:
      # The log level should not be decreased to INFO/DEBUG,
      # or only temporarily, as the airflow sheduler is very verbose,
      # putting a strain on the shared logging system
      # with WARN, the webserver currently prints a large json schema
      logging_level: ERROR
      remote_logging: "True"
      remote_base_log_folder: ~
      remote_log_conn_id: gcs_logs
    api:
      auth_backends: airflow.api.auth.backend.basic_auth, airflow.api.auth.backend.session
      workers: 1
      web_server_worker_timeout: 300
      # Prevent airflow to be rendered in an external web frame
      x_frame_enabled: "False"
    webserver:
      # Dev, Test and Prod are all external facing
      warn_deployment_exposure: "False"

    smtp:
      smtp_host: apps.smtp.gov.bc.ca
      smtp_starttls: false
      smtp_mail_from: cas-airflow@gov.bc.ca

    scheduler:
      min_file_process_interval: 30

    kubernetes_executor:
      run_as_user: "{{ .Values.uid }}"
      delete_worker_pods: "True"

  # Git sync
  dags:
    persistence:
      enabled: false
    gitSync:
      enabled: false

  statsd:
    enabled: false

devops:
  image:
    repository: hashicorp/terraform
    pullPolicy: Always
    tag: "1.4.6"

  resources:
    limits:
      cpu: 1000m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 64Mi

terraform-bucket-provision:
  terraform:
    namespace_apps: '["airflow-backups", "airflow-logs"]'
